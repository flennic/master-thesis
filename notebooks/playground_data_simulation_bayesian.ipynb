{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Simulation (Bayesian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to write general functions which can then be invoked for conducting the bayesian data simulation. It therefore is some kind of test field where each function is built on it's one, start from kernel construction, simulation, and so on until a whole data set has been simulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gp_utilities as gp_utils\n",
    "from gp_utilities import *\n",
    "import os\n",
    "import logging\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "# NEW\n",
    "import tensorflow as tf\n",
    "#tf.random.set_seed(42)\n",
    "\n",
    "# COMPAT\n",
    "#import tensorflow.compat.v1 as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "tfk = tfp.math.psd_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK :)\n",
    "def get_kernel_bayesian(smooth_amplitude, smooth_length_scale,\n",
    "                    periodic_amplitude, periodic_length_scale, periodic_period, periodic_local_amplitude, periodic_local_length_scale,\n",
    "                    global_periodic_amplitude, global_periodic_length_scale, global_periodic_period,\n",
    "                    irregular_amplitude, irregular_length_scale, irregular_scale_mixture,\n",
    "                    matern_onehalf_amplitude, matern_onehalf_length_scale,\n",
    "                    matern_threehalves_amplitude, matern_threehalves_length_scale,\n",
    "                    matern_fivehalves_amplitude, matern_fivehalves_length_scale):\n",
    "    \n",
    "    # Smooth kernel\n",
    "    smooth_kernel = tfk.ExponentiatedQuadratic(\n",
    "        amplitude=smooth_amplitude,\n",
    "        length_scale=smooth_length_scale)\n",
    "    \n",
    "    # Local periodic kernel\n",
    "    local_periodic_kernel = (\n",
    "        tfk.ExpSinSquared(\n",
    "            amplitude=periodic_amplitude, \n",
    "            length_scale=periodic_length_scale,\n",
    "            period=periodic_period) * \n",
    "        tfk.ExponentiatedQuadratic(\n",
    "            amplitude=periodic_local_amplitude,\n",
    "            length_scale=periodic_local_length_scale))\n",
    "    \n",
    "    # Periodic\n",
    "    global_periodic_kernel = tfk.ExpSinSquared(\n",
    "            amplitude=global_periodic_amplitude, \n",
    "            length_scale=global_periodic_length_scale,\n",
    "            period=global_periodic_period)\n",
    "    \n",
    "    # Irregular kernel\n",
    "    irregular_kernel = tfk.RationalQuadratic(\n",
    "        amplitude=irregular_amplitude,\n",
    "        length_scale=irregular_length_scale,\n",
    "        scale_mixture_rate=irregular_scale_mixture)\n",
    "    \n",
    "    # Matern 1/2\n",
    "    matern_onehalf_kernel = tfk.MaternOneHalf(\n",
    "        amplitude = matern_onehalf_amplitude,\n",
    "        length_scale = matern_onehalf_length_scale\n",
    "    )\n",
    "    \n",
    "    # Matern 3/2\n",
    "    matern_fivehalf_kernel = tfk.MaternThreeHalves(\n",
    "        amplitude = matern_threehalves_amplitude,\n",
    "        length_scale = matern_threehalves_length_scale\n",
    "    )\n",
    "    \n",
    "    # Matern 5/2\n",
    "    matern_fivehalf_kernel = tfk.MaternFiveHalves(\n",
    "        amplitude = matern_fivehalves_amplitude,\n",
    "        length_scale = matern_fivehalves_length_scale\n",
    "    )\n",
    "    \n",
    "    return smooth_kernel + local_periodic_kernel + irregular_kernel + matern_onehalf_kernel + matern_fivehalf_kernel + matern_fivehalf_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK :)\n",
    "def build_gp(X_train, Y_train):\n",
    "\n",
    "    #X_train = np.array(X_train).astype(float).reshape(-1, 1)\n",
    "    #Y_train = np.array(Y_train)\n",
    "    \n",
    "    def build_gp_internal(smooth_amplitude, smooth_length_scale,\n",
    "              periodic_amplitude, periodic_length_scale, periodic_period, periodic_local_amplitude, periodic_local_length_scale,\n",
    "              global_periodic_amplitude, global_periodic_length_scale, global_periodic_period,\n",
    "              irregular_amplitude, irregular_length_scale, irregular_scale_mixture,\n",
    "              matern_onehalf_amplitude, matern_onehalf_length_scale,\n",
    "              matern_threehalves_amplitude, matern_threehalves_length_scale,\n",
    "              matern_fivehalves_amplitude, matern_fivehalves_length_scale,\n",
    "              observation_noise_variance):\n",
    "        \"\"\"Defines the conditional dist. of GP outputs, given kernel parameters.\"\"\"\n",
    "        \n",
    "        # Create the covariance kernel, which will be shared between the prior (which we\n",
    "        # use for maximum likelihood training) and the posterior (which we use for\n",
    "        # posterior predictive sampling)\n",
    "        kernel = get_kernel_bayesian(smooth_amplitude, smooth_length_scale,\n",
    "                  periodic_amplitude, periodic_length_scale, periodic_period, periodic_local_amplitude, periodic_local_length_scale,\n",
    "                  global_periodic_amplitude, global_periodic_length_scale, global_periodic_period,\n",
    "                  irregular_amplitude, irregular_length_scale, irregular_scale_mixture,\n",
    "                  matern_onehalf_amplitude, matern_onehalf_length_scale,\n",
    "                  matern_threehalves_amplitude, matern_threehalves_length_scale,\n",
    "                  matern_fivehalves_amplitude, matern_fivehalves_length_scale)\n",
    "        \n",
    "        # Create the GP prior distribution, which we will use to train the model\n",
    "        # parameters.\n",
    "        return tfd.GaussianProcess(\n",
    "          mean_fn=lambda x: np.mean(Y_train),\n",
    "          kernel=kernel,\n",
    "          index_points=X_train,\n",
    "          observation_noise_variance=observation_noise_variance)\n",
    "    \n",
    "    return build_gp_internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK :)\n",
    "def prior_joint_model(X_train, Y_train, prior_dict=None):\n",
    "    \n",
    "    if prior_dict==None:\n",
    "        prior_dict = {\n",
    "            'smooth_amplitude': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'smooth_length_scale': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'periodic_amplitude': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'periodic_length_scale': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'periodic_period': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'periodic_local_amplitude': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'periodic_local_length_scale': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'global_periodic_amplitude': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'global_periodic_length_scale': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'global_periodic_period': tfd.LogNormal(loc=2., scale=np.float64(1)),\n",
    "            'irregular_amplitude': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'irregular_length_scale': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'irregular_scale_mixture': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'matern_onehalf_amplitude': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'matern_onehalf_length_scale': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'matern_threehalves_amplitude': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'matern_threehalves_length_scale': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'matern_fivehalves_amplitude': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'matern_fivehalves_length_scale': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "            'observation_noise_variance': tfd.LogNormal(loc=0., scale=np.float64(1)),\n",
    "        }\n",
    "    \n",
    "    prior_gp = build_gp(X_train, Y_train)\n",
    "    prior_dict['observations'] = prior_gp\n",
    "    \n",
    "    return tfd.JointDistributionNamed(prior_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_log_prob_from_joint(joint, Y_train):\n",
    "\n",
    "    # Use `tf.function` to trace the loss for more efficient evaluation.\n",
    "    @tf.function(autograph=False, experimental_compile=False)\n",
    "    def target_log_prob(smooth_amplitude,\n",
    "                       smooth_length_scale,\n",
    "                       periodic_amplitude,\n",
    "                       periodic_length_scale,\n",
    "                       periodic_period,\n",
    "                       periodic_local_amplitude,\n",
    "                       periodic_local_length_scale,\n",
    "                       global_periodic_amplitude,\n",
    "                       global_periodic_length_scale,\n",
    "                       global_periodic_period,\n",
    "                       irregular_amplitude,\n",
    "                       irregular_length_scale,\n",
    "                       irregular_scale_mixture,\n",
    "                       matern_onehalf_amplitude,\n",
    "                       matern_onehalf_length_scale,\n",
    "                       matern_threehalves_amplitude,\n",
    "                       matern_threehalves_length_scale,\n",
    "                       matern_fivehalves_amplitude,\n",
    "                       matern_fivehalves_length_scale,\n",
    "                       observation_noise_variance):\n",
    "        \n",
    "        return joint.log_prob({\n",
    "          'smooth_amplitude': smooth_amplitude,\n",
    "          'smooth_length_scale': smooth_length_scale,\n",
    "          'periodic_amplitude': periodic_amplitude,\n",
    "          'periodic_length_scale': periodic_length_scale,\n",
    "          'periodic_period': periodic_period,\n",
    "          'periodic_local_amplitude': periodic_local_amplitude,\n",
    "          'periodic_local_length_scale': periodic_local_length_scale,\n",
    "          'global_periodic_amplitude': global_periodic_amplitude,\n",
    "          'global_periodic_length_scale': global_periodic_length_scale,\n",
    "          'global_periodic_period': global_periodic_period,\n",
    "          'irregular_amplitude': irregular_amplitude,\n",
    "          'irregular_length_scale': irregular_length_scale,\n",
    "          'irregular_scale_mixture': irregular_scale_mixture,\n",
    "          'matern_onehalf_amplitude': matern_onehalf_amplitude,\n",
    "          'matern_onehalf_length_scale': matern_onehalf_length_scale,\n",
    "          'matern_threehalves_amplitude': matern_threehalves_amplitude,\n",
    "          'matern_threehalves_length_scale': matern_threehalves_length_scale,\n",
    "          'matern_fivehalves_amplitude': matern_fivehalves_amplitude,\n",
    "          'matern_fivehalves_length_scale': matern_fivehalves_length_scale,\n",
    "          'observation_noise_variance': observation_noise_variance,\n",
    "          'observations': Y_train\n",
    "        })\n",
    "    \n",
    "    return target_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # Speed up sampling by tracing with `tf.function`.\n",
    "    @tf.function(autograph=False, experimental_compile=False)#, experimental_relax_shapes=True)\n",
    "    def do_sampling(adaptive_sampler, initial_state, num_results=tf.constant(100), num_burnin_steps=tf.constant(500), parallel_iterations=tf.constant(10)):\n",
    "\n",
    "        return tfp.mcmc.sample_chain(\n",
    "          kernel=adaptive_sampler,\n",
    "          current_state=initial_state,\n",
    "          num_results=num_results,\n",
    "          num_burnin_steps=num_burnin_steps,\n",
    "          parallel_iterations=parallel_iterations,\n",
    "          trace_fn=lambda current_state, kernel_results: kernel_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_to_posterior(theta):\n",
    "    \n",
    "    if theta is None:\n",
    "        return None\n",
    "    #print(\"Actually constructing the posterior!\")\n",
    "    \n",
    "    posterior = {\n",
    "        'smooth_amplitude': tfd.Empirical(theta[0]),\n",
    "        'smooth_length_scale': tfd.Empirical(theta[1]),\n",
    "        'periodic_amplitude': tfd.Empirical(theta[2]),\n",
    "        'periodic_length_scale': tfd.Empirical(theta[3]),\n",
    "        'periodic_period': tfd.Empirical(theta[4]),\n",
    "        'periodic_local_amplitude': tfd.Empirical(theta[5]),\n",
    "        'periodic_local_length_scale': tfd.Empirical(theta[6]),\n",
    "        'global_periodic_amplitude': tfd.Empirical(theta[7]),\n",
    "        'global_periodic_length_scale': tfd.Empirical(theta[8]),\n",
    "        'global_periodic_period': tfd.Empirical(theta[9]),\n",
    "        'irregular_amplitude': tfd.Empirical(theta[10]),\n",
    "        'irregular_length_scale': tfd.Empirical(theta[11]),\n",
    "        'irregular_scale_mixture': tfd.Empirical(theta[12]),\n",
    "        'matern_onehalf_amplitude': tfd.Empirical(theta[13]),\n",
    "        'matern_onehalf_length_scale': tfd.Empirical(theta[14]),\n",
    "        'matern_threehalves_amplitude': tfd.Empirical(theta[15]),\n",
    "        'matern_threehalves_length_scale': tfd.Empirical(theta[16]),\n",
    "        'matern_fivehalves_amplitude': tfd.Empirical(theta[17]),\n",
    "        'matern_fivehalves_length_scale': tfd.Empirical(theta[18]),\n",
    "        'observation_noise_variance': tfd.Empirical(theta[19]),\n",
    "    }\n",
    "    \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(X, Y, step_size, num_results, num_leapfrog_steps, num_burnin_steps, constrain_positive, target_accept_prob, parallel_iterations, theta=None):\n",
    "    \n",
    "    posterior = theta_to_posterior(theta)\n",
    "    \n",
    "    # Create Joint Model\n",
    "    joint = prior_joint_model(X, Y, posterior)\n",
    "    \n",
    "    # Target Log Prob\n",
    "    target_log_prob = target_log_prob_from_joint(joint, Y)\n",
    "\n",
    "    # Create sampler\n",
    "    sampler = tfp.mcmc.TransformedTransitionKernel(\n",
    "        tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=target_log_prob,\n",
    "            step_size=tf.cast(step_size, tf.float64),\n",
    "            num_leapfrog_steps=num_leapfrog_steps),\n",
    "            bijector=[constrain_positive]*20)\n",
    "\n",
    "    adaptive_sampler = tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
    "        inner_kernel=sampler,\n",
    "        num_adaptation_steps=int(0.8 * num_burnin_steps),\n",
    "        target_accept_prob=tf.cast(target_accept_prob, tf.float64))\n",
    "\n",
    "    # Initial States for pyhisical walk\n",
    "    if theta == None:\n",
    "        nitial_state = tf.constant([1.]*20, tf.float64)\n",
    "        initial_state = [tf.cast(x, tf.float64) for x in [1.]*20]\n",
    "    else:\n",
    "        #initial_state = tf.constant([x[0] for x in theta], tf.float64)\n",
    "        initial_state = [tf.cast(x[0], tf.float64) for x in theta]\n",
    "        \n",
    "    #initial_state = tf.constant(initial_state, tf.float64)\n",
    "    \n",
    "    samples, kernel_results = do_sampling(adaptive_sampler,\n",
    "                                              initial_state,\n",
    "                                              num_results=num_results,\n",
    "                                              num_burnin_steps=num_burnin_steps,\n",
    "                                              parallel_iterations=parallel_iterations)\n",
    "    \n",
    "    return samples, kernel_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_slice(sslice, theta=None, padding=27_000//48//2, n=100, num_leapfrog_steps=8, step_size=0.1, num_results=100,\n",
    "                   num_burnin_steps=200, parallel_iterations=10, target_accept_prob=0.75, predictive_noise_variance=0., num_predictive_points=200):\n",
    "    \n",
    "    \n",
    "    # Preparation\n",
    "    X=sslice[\"Recording\"][\"ContractionNoNorm\"]\n",
    "    Y=sslice[\"Recording\"][\"RrInterval\"]\n",
    "    X = np.array(X).astype(float).reshape(-1, 1)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    ### TEST\n",
    "    X = tf.constant(X)\n",
    "    Y = tf.constant(Y)\n",
    "    ### TEST\n",
    "    \n",
    "    constrain_positive = tfb.Shift(np.finfo(np.float64).tiny)(tfb.Exp())\n",
    "    \n",
    "    #print(\"Starting Trial and Error\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        try:\n",
    "            samples, kernel_results = sample(X, Y,\n",
    "                                     step_size,\n",
    "                                     num_results,\n",
    "                                     num_leapfrog_steps,\n",
    "                                     num_burnin_steps,\n",
    "                                     constrain_positive,\n",
    "                                     target_accept_prob,\n",
    "                                     parallel_iterations,\n",
    "                                     theta=theta)\n",
    "            break\n",
    "        except tf.errors.InvalidArgumentError:\n",
    "            if i >= 4:\n",
    "                print(f\"Cholesky decompositon error detected 5 times now. Trying one last time with inital priors.\")\n",
    "                samples, kernel_results = sample(X, Y,\n",
    "                                                 step_size,\n",
    "                                                 num_results,\n",
    "                                                 num_leapfrog_steps,\n",
    "                                                 num_burnin_steps,\n",
    "                                                 constrain_positive,\n",
    "                                                 target_accept_prob,\n",
    "                                                 parallel_iterations,\n",
    "                                                 theta=None)\n",
    "                break\n",
    "            print(f\"Cholesky decompositon error detected in run {i+1}. Retrying.\")                            \n",
    "            #continue\n",
    "            #break\n",
    "    \n",
    "    #print(\"End\")\n",
    "            \n",
    "\n",
    "   # print(\"Cholesky Error detected 3 times, trying one last time withtaking default priors.\")\n",
    "   #             samples, kernel_results = sample(X, Y,\n",
    "   #                                      step_size,\n",
    "   #                                      num_results,\n",
    "   #                                      num_leapfrog_steps,\n",
    "   #                                      num_burnin_steps,\n",
    "   #                                      constrain_positive,\n",
    "   #                                      target_accept_prob,\n",
    "   #                                      parallel_iterations,\n",
    "   #                                      theta=None)\n",
    "\n",
    "    (smooth_amplitude_samples,\n",
    "    smooth_length_scale_samples,\n",
    "    periodic_amplitude_samples,\n",
    "    periodic_length_scale_samples,\n",
    "    periodic_period_samples,\n",
    "    periodic_local_amplitude_samples,\n",
    "    periodic_local_length_scale_samples,\n",
    "    global_periodic_amplitude_samples,\n",
    "    global_periodic_length_scale_samples,\n",
    "    global_periodic_period_samples,\n",
    "    irregular_amplitude_samples,\n",
    "    irregular_length_scale_samples,\n",
    "    irregular_scale_mixture_samples,\n",
    "    matern_onehalf_amplitude_samples,\n",
    "    matern_onehalf_scale_samples,\n",
    "    matern_threehalves_amplitude_samples,\n",
    "    matern_threehalves_scale_samples,\n",
    "    matern_fivehalves_amplitude_samples,\n",
    "    matern_fivehalves_scale_samples,\n",
    "    observation_noise_variance_samples) = samples\n",
    "    \n",
    "    # Given the posterior parameter and thus kernel samples, we can use that to fit the GP and predict for the posterior points\n",
    "    predictive_index_points_ = np.linspace(0, padding-2, num_predictive_points, dtype=np.float64).reshape(-1, 1)\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        # The sampled hyperparams have a leading batch dimension, `[num_results, ...]`,\n",
    "        # so they construct a *batch* of kernels.\n",
    "        batch_of_posterior_kernels = get_kernel_bayesian(smooth_amplitude_samples,\n",
    "                                                    smooth_length_scale_samples,\n",
    "                                                    periodic_amplitude_samples,\n",
    "                                                    periodic_length_scale_samples,\n",
    "                                                    periodic_period_samples,\n",
    "                                                    periodic_local_amplitude_samples,\n",
    "                                                    periodic_local_length_scale_samples,\n",
    "                                                    global_periodic_amplitude_samples,\n",
    "                                                    global_periodic_length_scale_samples,\n",
    "                                                    global_periodic_period_samples,\n",
    "                                                    irregular_amplitude_samples,\n",
    "                                                    irregular_length_scale_samples,\n",
    "                                                    irregular_scale_mixture_samples,\n",
    "                                                    matern_onehalf_amplitude_samples,\n",
    "                                                    matern_onehalf_scale_samples,\n",
    "                                                    matern_threehalves_amplitude_samples,\n",
    "                                                    matern_threehalves_scale_samples,\n",
    "                                                    matern_fivehalves_amplitude_samples,\n",
    "                                                    matern_fivehalves_scale_samples)\n",
    "\n",
    "        # The batch of kernels creates a batch of GP predictive models, one for each\n",
    "        # posterior sample.\n",
    "        batch_gprm = tfd.GaussianProcessRegressionModel(\n",
    "            mean_fn=lambda x: np.mean(Y),\n",
    "            kernel=batch_of_posterior_kernels,\n",
    "            index_points=predictive_index_points_,\n",
    "            observation_index_points=X,\n",
    "            observations=Y,\n",
    "            observation_noise_variance=observation_noise_variance_samples,\n",
    "            predictive_noise_variance=predictive_noise_variance)\n",
    "\n",
    "        # To construct the marginal predictive distribution, we average with uniform\n",
    "        # weight over the posterior samples.\n",
    "        predictive_gprm = tfd.MixtureSameFamily(\n",
    "            mixture_distribution=tfd.Categorical(logits=tf.zeros([num_results])),\n",
    "            components_distribution=batch_gprm)\n",
    "\n",
    "        simulated = predictive_gprm.sample(n)\n",
    "    \n",
    "    return simulated, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_in_recordings(recordings, in_seconds=True, low_rri: int = 300, high_rri: int = 2000, ectopic_beats_removal_method: str = \"kamath\"):\n",
    "    \n",
    "    cleared_recordings = []\n",
    "    recordings_deepcopy = copy.deepcopy(recordings)\n",
    "    denom = 1000 if in_seconds else 1\n",
    "    \n",
    "    for i, recording in enumerate(recordings_deepcopy):\n",
    "        \n",
    "        # Status\n",
    "        #print(str(round(i/len(recordings_deepcopy)*100, 2)) + \"%\", end=\"\\r\")\n",
    "        \n",
    "        rrinterval = hrv.remove_outliers(recording[\"Recording\"][\"RrInterval\"],\n",
    "                                                               low_rri=low_rri, high_rri=high_rri, verbose=False)\n",
    "        \n",
    "        rrinterval = hrv.remove_ectopic_beats(rrinterval,\n",
    "                                        method=ectopic_beats_removal_method, verbose=False)\n",
    "        \n",
    "        recording[\"Recording\"][\"RrInterval\"] = np.array(rrinterval) / denom\n",
    "        \n",
    "        recording[\"Recording\"].dropna(inplace=True)\n",
    "        \n",
    "        cleared_recordings.append(recording)\n",
    "        \n",
    "    return cleared_recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_splice_lod_constant_by_number(lod, n=48):\n",
    "    spliced_recordings = []\n",
    "    \n",
    "    for i, recording in enumerate(lod):\n",
    "        \n",
    "        # Status\n",
    "        #print(str(round(i/len(lod)*100, 2)) + \"%\", end=\"\\r\")\n",
    "        \n",
    "        splices = np.array_split(recording[\"Recording\"], n)\n",
    "        \n",
    "        for splice in splices:\n",
    "            recording_deepcopy = copy.deepcopy(recording)\n",
    "            recording_deepcopy[\"Recording\"] = splice\n",
    "            recording_deepcopy[\"Recording\"][\"ContractionNoNorm\"] = list(range(len(recording_deepcopy[\"Recording\"][\"ContractionNoNorm\"])))\n",
    "            spliced_recordings.append(recording_deepcopy)\n",
    "    \n",
    "    return spliced_recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "import utilities as utils\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_gdansk = \"../data/age_decades/\"\n",
    "input_physionet = \"/home/flennic/Downloads/physionet.org/files/crisdb/1.0.0/\"\n",
    "subdirs_physionet = [\"e/\", \"f/\", \"m/\"]\n",
    "output_dir = \"../data/preprocessed/\"\n",
    "\n",
    "data_source_type = \"gdansk\" # [\"gdansk\", \"physionet\"]\n",
    "splice_type = \"constant\" # [\"complete\", \"constant\", \"random\"]\n",
    "label_type = \"regression\" # [\"classification\", \"regression\"]\n",
    "output_type = \"deep\" # [\"features\", \"deep\"]\n",
    "in_seconds = True # Will be automatically set to false for creating features, as the frequency features require milli seconds.\n",
    "\n",
    "splits_gdansk = [0.6, 0.2, 0.2]\n",
    "splits_physionet = [0.8, 0.1, 0.1]\n",
    "seed = 42\n",
    "pad_length = 27_000 if data_source_type == \"gdansk\" else 135_000 # For deep learning padding\n",
    "N = 48 if data_source_type == \"gdansk\" else 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto adjustment\n",
    "if data_source_type == \"gdansk\":\n",
    "    splits = splits_gdansk\n",
    "elif data_source_type == \"physionet\":\n",
    "    splits = splits_physionet\n",
    "else:\n",
    "    raise Exception(\"Data source not supported.\")\n",
    "    \n",
    "if output_type == \"features\":\n",
    "    in_seconds = False\n",
    "    unit = \"milliseconds\"\n",
    "else:\n",
    "    unit = \"seconds\" if in_seconds else \"milliseconds\"\n",
    "    \n",
    "if splice_type == \"constant\":\n",
    "    pad_length //= N\n",
    "    \n",
    "np.random.seed(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.3 s, sys: 71.3 ms, total: 13.4 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if data_source_type == \"gdansk\":\n",
    "    recordings = utils.read_gdansk_to_dict(input_gdansk)    \n",
    "elif data_source_type == \"physionet\":\n",
    "    recordings = utils.read_physionet_to_dict(input_physionet, subdirs_physionet)\n",
    "else:\n",
    "    raise Exception(\"Data source not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "cleared_recordings = remove_outliers_in_recordings(recordings, in_seconds=in_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 138 µs, total: 138 µs\n",
      "Wall time: 134 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_recordings, val_recordings, test_recordings = np.array_split(recordings, (np.array(splits)[:-1].cumsum() * len(recordings)).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recordings_cleared = remove_outliers_in_recordings(train_recordings)\n",
    "val_recordings_cleared = remove_outliers_in_recordings(val_recordings)\n",
    "test_recordings_cleared = remove_outliers_in_recordings(test_recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 s, sys: 87.6 ms, total: 11.4 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_recordings = c_splice_lod_constant_by_number(train_recordings, n=2*N)\n",
    "val_recordings = c_splice_lod_constant_by_number(val_recordings, n=2*N)\n",
    "test_recordings = c_splice_lod_constant_by_number(test_recordings, n=2*N)\n",
    "\n",
    "del train_recordings_cleared, val_recordings_cleared, test_recordings_cleared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(train_recordings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_recordings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = train_recordings[0][\"Recording\"][\"ContractionNoNorm\"]\n",
    "Y = train_recordings[0][\"Recording\"][\"RrInterval\"]\n",
    "X2 = train_recordings[1][\"Recording\"][\"ContractionNoNorm\"]\n",
    "Y2 = train_recordings[1][\"Recording\"][\"RrInterval\"]\n",
    "given_slice = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "given_slice2 = pd.DataFrame({\"X\": X2, \"Y\": Y2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(X, Y);\n",
    "sns.lineplot(X2, Y2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "simulated_data, theta = simulate_slice(given_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.lineplot(X, Y);\n",
    "sns.lineplot(range(len(simulated_data[0,:])), simulated_data[5,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "simulated_data_Post, theta_Post = simulate_slice(given_slice, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.lineplot(X, Y);\n",
    "sns.lineplot(range(len(simulated_data[0,:])), simulated_data_Post[5,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "simulated_data_Post2, theta_Post2 = simulate_slice(given_slice, theta_Post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.lineplot(X, Y);\n",
    "sns.lineplot(range(len(simulated_data[0,:])), simulated_data_Post2[5,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "simulated_data2, theta2 = simulate_slice(given_slice2, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simulated_data2_None, theta2_None = simulate_slice(given_slice2, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING:tensorflow:5 out of the last 5 calls to <function do_sampling at 0x7fa8f33ed170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.lineplot(range(len(simulated_data2[0,:])), simulated_data2[0,:]);\n",
    "sns.lineplot(range(len(simulated_data2_None[0,:])), simulated_data2_None[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_generator(recordings, padding=27_000//48//2,\n",
    "                                     n=100,\n",
    "                                     num_leapfrog_steps=8,\n",
    "                                     step_size=0.1,\n",
    "                                     num_results=100,\n",
    "                                     num_burnin_steps=200,\n",
    "                                     parallel_iterations=10,\n",
    "                                     target_accept_prob=0.75,\n",
    "                                     predictive_noise_variance=0.,\n",
    "                                     num_predictive_points=200):\n",
    "    \n",
    "    theta = None\n",
    "    \n",
    "    for recording in recordings:\n",
    "        simulated_data, theta = simulate_slice(recording,\n",
    "                                               theta=theta,\n",
    "                                               padding=padding,\n",
    "                                               n=n,\n",
    "                                               num_leapfrog_steps=num_leapfrog_steps,\n",
    "                                               step_size=step_size,\n",
    "                                               num_results=num_results,\n",
    "                                               num_burnin_steps=num_burnin_steps,\n",
    "                                               parallel_iterations=parallel_iterations,\n",
    "                                               target_accept_prob=target_accept_prob,\n",
    "                                               predictive_noise_variance=predictive_noise_variance,\n",
    "                                               num_predictive_points=num_predictive_points)\n",
    "        yield recording, simulated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def file_len(fname):\n",
    "    p = subprocess.Popen(['wc', '-l', fname], stdout=subprocess.PIPE, \n",
    "                                              stderr=subprocess.PIPE)\n",
    "    result, err = p.communicate()\n",
    "    if p.returncode != 0:\n",
    "        raise IOError(err)\n",
    "    return int(result.strip().split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_df_list_to_files(dfs, basedir, set_selector=\"train\", padding=200, append=False, start_index=0):\n",
    "\n",
    "    append_df_list_to_file_feature(dfs, basedir, set_selector, append, start_index)\n",
    "    append_df_list_to_file_deep(dfs, basedir, set_selector, padding, append, start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_df_list_to_file_feature(dfs, basedir, set_selector=\"train\", append=False, start_index=0):\n",
    "    \n",
    "    header = not append\n",
    "    mode = 'a' if append else 'w'\n",
    "    indeces = list(range(start_index, start_index+len(dfs)))\n",
    "    \n",
    "    for df in dfs:\n",
    "        df[\"Recording\"][\"RrInterval\"] = df[\"Recording\"][\"RrInterval\"] * 1000.0\n",
    "        \n",
    "    #print(dfs[0])\n",
    "    \n",
    "    dfs_classification = utils.recordings_to_feature_dataframe(dfs, classification=True)\n",
    "    dfs_regression = utils.recordings_to_feature_dataframe(dfs, classification=False)\n",
    "    \n",
    "    dfs_classification.index = indeces\n",
    "    dfs_regression.index = indeces\n",
    "    \n",
    "    dfs_classification.to_csv(f'{basedir}gdansk_simulated_constant_features_classification_milliseconds_original_{set_selector}.csv', mode=mode, header=header)\n",
    "    dfs_regression.to_csv(f'{basedir}gdansk_simulated_constant_features_regression_milliseconds_original_{set_selector}.csv', mode=mode, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_df_list_to_file_deep(dfs, basedir, set_selector=\"train\", padding=200, append=False, start_index=0):\n",
    "    \n",
    "    header = not append\n",
    "    mode = 'a' if append else 'w'\n",
    "    indeces = list(range(start_index, start_index+len(dfs)))\n",
    "    #print(indeces)\n",
    "    \n",
    "    dfs_classification = utils.recordings_to_deep_dataframe(dfs, pad_length=padding, classification=True, gdansk=True)\n",
    "    dfs_regression = utils.recordings_to_deep_dataframe(dfs, pad_length=padding, classification=False, gdansk=True)\n",
    "    \n",
    "    dfs_classification.index = indeces\n",
    "    dfs_regression.index = indeces\n",
    "    \n",
    "    dfs_classification.to_csv(f'{basedir}gdansk_simulated_constant_deep_classification_seconds_original_{set_selector}.csv', mode=mode, header=header)\n",
    "    dfs_regression.to_csv(f'{basedir}gdansk_simulated_constant_deep_regression_seconds_original_{set_selector}.csv', mode=mode, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_1d_to_recording(tensor, start_index=0):\n",
    "    \n",
    "    series = pd.Series(tensor)\n",
    "    indeces = list(range(start_index, start_index+len(series)))\n",
    "    \n",
    "    df = pd.DataFrame({\"ContractionNo\": list(range(len(series))),\n",
    "                       \"ContractionNoNorm\": list(range(len(series))),\n",
    "                       \"RrInterval\": series})\n",
    "    \n",
    "    df.index = indeces\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor_1d_to_recording(tf.constant([[0,1,2,3,4,5,6,7,8,9], [10,11,12,13,14,15,16,17,18,19]])[1,:], start_index=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a memory leak test.\n",
    "def simulate_data_memory_leak(recordings):\n",
    "    my_generator = simulation_generator(recordings, n=100)\n",
    "    for i, item in enumerate(my_generator):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#sslice, theta=None, padding=27_000//48//2, n=100, num_leapfrog_steps=8, step_size=0.1, num_results=100,\n",
    "#                   num_burnin_steps=200, parallel_iterations=10, target_accept_prob=0.75, predictive_noise_variance=0., num_predictive_points=200\n",
    "from memory_profiler import profile\n",
    "@profile(precision=6)\n",
    "def simulate_data(orig_recordings,\n",
    "                  set_selector=\"train\",\n",
    "                  basedir=\"../data/preprocessed/\",\n",
    "                  padding=27_000//48//2,\n",
    "                  num_leapfrog_steps=8,\n",
    "                  step_size=0.1,\n",
    "                  n=10,\n",
    "                  num_results=100,\n",
    "                  num_burnin_steps=100,\n",
    "                  parallel_iterations=10,\n",
    "                  target_accept_prob=0.75,\n",
    "                  predictive_noise_variance=0.,\n",
    "                  num_predictive_points=200):\n",
    "    \n",
    "    path_feature_classification = f\"{basedir}gdansk_simulated_constant_features_classification_milliseconds_original_{set_selector}.csv\"\n",
    "    path_feature_regression = f\"{basedir}gdansk_simulated_constant_features_regression_milliseconds_original_{set_selector}.csv\"\n",
    "    path_deep_classification = f\"{basedir}gdansk_simulated_constant_deep_classification_seconds_original_{set_selector}.csv\"\n",
    "    path_deep_regression = f\"{basedir}gdansk_simulated_constant_deep_regression_seconds_original_{set_selector}.csv\"\n",
    "    \n",
    "    output_paths = [path_feature_classification, path_feature_regression, path_deep_classification, path_deep_regression]\n",
    "    #print(output_paths)\n",
    "    \n",
    "    # Determine where to start\n",
    "    start = 0\n",
    "    if all(map(path.exists, output_paths)):\n",
    "        # Continue\n",
    "        #print(\"CONTINUEE\")\n",
    "        file_lengths = list(map(file_len, output_paths))\n",
    "        #print(file_lengths)\n",
    "        if len(set(file_lengths)) == 1:\n",
    "            start = list(file_lengths)[0]//n\n",
    "    \n",
    "    append = start != 0\n",
    "    total_count = len(orig_recordings)\n",
    "    \n",
    "    print(f\"Processing {start+1}/{total_count}\")\n",
    "    \n",
    "    # If not everything is sane, raise an exception\n",
    "    if os.path.exists(path_feature_classification) and start == 0:\n",
    "        raise Exception(f\"For safety reasons, file {path_feature_classification} must be deleted manually.\")\n",
    "        \n",
    "    if os.path.exists(path_feature_regression) and start == 0:\n",
    "        raise Exception(f\"For safety reasons, file {path_feature_regression} must be deleted manually.\")\n",
    "        \n",
    "    if os.path.exists(path_deep_classification) and start == 0:\n",
    "        raise Exception(f\"For safety reasons, file {path_deep_classification} must be deleted manually.\")\n",
    "        \n",
    "    if os.path.exists(path_deep_regression) and start == 0:\n",
    "        raise Exception(f\"For safety reasons, file {path_deep_regression} must be deleted manually.\")\n",
    "    \n",
    "    orig_recordings = orig_recordings[start:]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, res in enumerate(simulation_generator(orig_recordings, n=n)):\n",
    "\n",
    "        orig_recording, simulated_data = res\n",
    "        \n",
    "        #print(orig_recording)\n",
    "        #print(simulated_data)\n",
    "        print(\"Data simulated, creating list of dataframes.\")\n",
    "        \n",
    "        simulated_dfs = []\n",
    "        \n",
    "        for j in range(simulated_data.shape[0]):\n",
    "            \n",
    "            simulated = simulated_data[j,:]\n",
    "            \n",
    "            recording_deepcopy = copy.deepcopy(orig_recording)\n",
    "            recording_deepcopy[\"Recording\"] = tensor_1d_to_recording(simulated)\n",
    "            simulated_dfs.append(recording_deepcopy)\n",
    "        \n",
    "        print(\"List of dataframes created. Saving results to file.\")\n",
    "        # 2 + 9*10\n",
    "        #print(f\"{start*n} + {i}*{n}\")\n",
    "        index_continue = start*n + i*n\n",
    "        #print(f\"##### {index_continue}\")\n",
    "        print(simulated_dfs[0])\n",
    "        append_df_list_to_files(simulated_dfs, basedir, set_selector, padding=200, append=append, start_index=index_continue)\n",
    "        \n",
    "        time_diff = time.time() - start_time\n",
    "        time_remaining_in_hours = (time_diff*(len(orig_recordings)-i))//3600\n",
    "        \n",
    "        print(f\"Time taken: {round(time_diff)} seconds\")\n",
    "        print(f\"Estimated time remaining: {round(time_remaining_in_hours)} hours\")\n",
    "        print(f\"Estimated time finished: {datetime.now() + timedelta(hours=time_remaining_in_hours)}\")\n",
    "        print(\"\\n\\n\")\n",
    "        print(f\"Processing {start+(i+2)}/{total_count} next, if available, or end.\")\n",
    "        \n",
    "        append=True\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if i >= 20:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_recordings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_selector = \"train\" # [\"train\", \"val\", \"test\"]\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_recordings[64]['Recording']['RrInterval']) # 227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14311    1208.0\n",
       "14312    1232.0\n",
       "14313    1224.0\n",
       "14314    1224.0\n",
       "14315    1240.0\n",
       "          ...  \n",
       "14533    1256.0\n",
       "14534    1264.0\n",
       "14535    1256.0\n",
       "14536    1248.0\n",
       "14537    1264.0\n",
       "Name: RrInterval, Length: 227, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_recordings[63]['Recording']['RrInterval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14538    1272.0\n",
       "14539    1296.0\n",
       "14540    1280.0\n",
       "14541    1280.0\n",
       "14542    1296.0\n",
       "          ...  \n",
       "14760    1192.0\n",
       "14761    1192.0\n",
       "14762    1200.0\n",
       "14763    1200.0\n",
       "14764    1208.0\n",
       "Name: RrInterval, Length: 227, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_recordings[64]['Recording']['RrInterval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file <ipython-input-27-2e0703cb0726>\n",
      "NOTE: %mprun can only be used on functions defined in physical files, and not in the IPython environment.\n",
      "Processing 437/10368\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "simulate_data(train_recordings, set_selector=set_selector, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate_data_memory_leak(train_recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
